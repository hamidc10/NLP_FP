{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports compelete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "project references: \n",
    "https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html\n",
    "https://medium.com/rapids-ai/faster-topic-modeling-with-bertopic-and-rapids-cuml-5c7559aba898\n",
    "https://towardsdatascience.com/topic-modeling-with-lsa-plsa-lda-nmf-bertopic-top2vec-a-comparison-5e6ce4b1e4a5\n",
    "https://hdbscan.readthedocs.io/en/latest/parameter_selection.html\n",
    "\"\"\"\n",
    "\n",
    "# installs\n",
    "# !pip install bertopic \n",
    "\n",
    "\"\"\"\n",
    "NOTE: to use GPU acclerated UMAP and HDBSCAN, you need to install RAPIDS cuML.\n",
    "Ensure you have the proper cupy version (cupy-cuda11x for Cheaha)\n",
    "For more installation info, see: https://docs.rapids.ai/install\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "\n",
    "# GPU accelerated\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import HDBSCAN\n",
    "\n",
    "# not GPU accelerated\n",
    "# from umap import UMAP\n",
    "# from hdbscan import HDBSCAN\n",
    "    \n",
    "print(\"imports compelete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "## Download and decompress data\n",
    "\n",
    "# uncomment below to initially download compressed data file\n",
    "# !wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFilesSmall/Gift_Cards_5.json.gz\n",
    "\n",
    "\n",
    "chunk_size=5 * 1024 * 1024\n",
    "\n",
    "\n",
    "with gzip.open(\"Gift_Cards_5.json.gz\", 'rb') as f:\n",
    "    with open(\"Gift_Cards_5.json\", 'wb') as f_out:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f_out.write(chunk)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 11.5 ms, sys: 2.69 ms, total: 14.2 ms\n",
      "Wall time: 45.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# store data in list\n",
    "\n",
    "training_data_gift_cards = []\n",
    "\n",
    "with open('Gift_Cards_5.json') as f:\n",
    "    for review in f:\n",
    "        review = json.loads(review)\n",
    "        text = review.get(\"reviewText\", \"\").strip()\n",
    "        summary = review.get(\"summary\", \"\").strip()\n",
    "        review = summary + \" \" + text\n",
    "        if review.strip():\n",
    "            training_data_gift_cards.append(review)\n",
    "            \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 93/93 [00:05<00:00, 17.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.14 s, sys: 1.58 s, total: 9.73 s\n",
      "Wall time: 25.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Generate embeddings for data, using a GPU if available\n",
    "\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' \n",
    "    print(\"using gpu\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "embeddings = model.encode(training_data_gift_cards, device=device, show_progress_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data amount: 2972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 20:46:00,604 - BERTopic - Reduced dimensionality\n",
      "2023-11-26 20:46:00,656 - BERTopic - Clustered reduced embeddings\n",
      "2023-11-26 20:46:01,069 - BERTopic - Reduced number of topics from 8 to 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained\n",
      "CPU times: user 2.72 s, sys: 77.9 ms, total: 2.8 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train and save model\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "print(f\"training data amount: {len(training_data_gift_cards)}\")\n",
    "\n",
    "\n",
    "umap_model = UMAP(random_state=42)\n",
    "\n",
    "# does clustering\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=45, min_samples=20, cluster_selection_epsilon=1)\n",
    "\n",
    "# tokenize, remove stopwords\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 1), stop_words=\"english\", min_df = 2)\n",
    "\n",
    "\n",
    "keybert_model = KeyBERTInspired()\n",
    "pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"POS\": pos_model\n",
    "}\n",
    "\n",
    "bert_model = BERTopic(\n",
    "    embedding_model=model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    top_n_words=10,\n",
    "    nr_topics=25,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = bert_model.fit_transform(training_data_gift_cards, embeddings)\n",
    "\n",
    "bert_model.save(\"bertopic_model_gift_cards_full\")\n",
    "\n",
    "print(\"model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                           Name  \\\n",
      "0     -1    261  -1_stars_worked_arrived_thank   \n",
      "1      0     61     0_great_stars_weekend_guft   \n",
      "2      1    148    1_stars_great_good_delivery   \n",
      "3      2     97     2_stars_loved_love_awesome   \n",
      "4      3   1620        3_gift_card_cards_great   \n",
      "5      4    634   4_stars_gift_christmas_great   \n",
      "6      5     53       5_perfect_pretty_good_ok   \n",
      "7      6     98   6_product_great_seller_stars   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [stars, worked, arrived, thank, upload, works,...   \n",
      "1  [great, stars, weekend, guft, hand, movies, go...   \n",
      "2  [stars, great, good, delivery, fast, worked, g...   \n",
      "3  [stars, loved, love, awesome, girlfriend, enjo...   \n",
      "4  [gift, card, cards, great, amazon, love, good,...   \n",
      "5  [stars, gift, christmas, great, cute, gifts, e...   \n",
      "6  [perfect, pretty, good, ok, ty, nice, great, s...   \n",
      "7  [product, great, seller, stars, deal, good, pu...   \n",
      "\n",
      "                                             KeyBERT  \\\n",
      "0  [stars, addition, 25, fantastic, outstanding, ...   \n",
      "1  [stars, great, weekend, friends, movies, time,...   \n",
      "2  [gift, stars, delivered, card, delivery, prese...   \n",
      "3  [stars, loved, liked, enjoyed, love, loves, mo...   \n",
      "4  [gift, gifts, card, cards, christmas, box, sho...   \n",
      "5  [gift, stars, gifts, star, giving, birthday, r...   \n",
      "6  [good, excellent, pretty, great, nice, thanks,...   \n",
      "7  [stars, star, product, seller, gift, purchase,...   \n",
      "\n",
      "                                                 MMR  \\\n",
      "0  [stars, worked, arrived, thank, upload, works,...   \n",
      "1  [great, stars, weekend, guft, hand, movies, go...   \n",
      "2  [stars, great, good, delivery, fast, worked, g...   \n",
      "3  [stars, loved, love, awesome, girlfriend, enjo...   \n",
      "4  [gift, card, cards, great, amazon, love, good,...   \n",
      "5  [stars, gift, christmas, great, cute, gifts, e...   \n",
      "6  [perfect, pretty, good, ok, ty, nice, great, s...   \n",
      "7  [product, great, seller, stars, deal, good, pu...   \n",
      "\n",
      "                                                 POS  \\\n",
      "0  [great, stocking, time, thanks, stuffer, deal,...   \n",
      "1  [great, weekend, guft, hand, movies, time, , ,...   \n",
      "2  [great, good, delivery, fast, gift, card, , , , ]   \n",
      "3          [awesome, girlfriend, good, , , , , , , ]   \n",
      "4  [gift, card, cards, great, amazon, good, use, ...   \n",
      "5  [stars, gift, christmas, great, cute, gifts, e...   \n",
      "6  [perfect, good, nice, great, low, balance, , ,...   \n",
      "7  [product, great, seller, deal, good, purchase,...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0    [Five Stars A+, Five Stars A+++, Five Stars A+]  \n",
      "1  [Five Stars Great, Five Stars great, Five Star...  \n",
      "2  [Five Stars great gift, Five Stars great gift,...  \n",
      "3  [Five Stars loved it, Five Stars Loved!, Five ...  \n",
      "4  [A gift card is a gift card... It's a gift car...  \n",
      "5  [Five Stars This was a gift, Five Stars This w...  \n",
      "6  [Pretty good! Pretty good!, Pretty good! Prett...  \n",
      "7  [Five Stars great product and great seller, Fi...  \n"
     ]
    }
   ],
   "source": [
    "# visualize results\n",
    "\n",
    "bert_model.visualize_topics().write_html(\"./intertopic_dist_gift_cards_full.html\")\n",
    "bert_model.visualize_barchart(top_n_topics = 25).write_html(\"./barchart_gift_cards_full.html\")\n",
    "bert_model.visualize_hierarchy().write_html(\"./hieararchy_gift_cards_full.html\")\n",
    "bert_model.visualize_documents(training_data_gift_cards).write_html(\"./projections_gift_cards_full.html\")\n",
    "\n",
    "print(bert_model.get_topic_info())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
