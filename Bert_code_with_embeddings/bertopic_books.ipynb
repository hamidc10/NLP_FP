{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation Instructions and References\n",
    "\n",
    "## Installation\n",
    "\n",
    "- BERTopic can be installed using pip.\n",
    "- To use GPU acceleration for UMAP and hDBSCAN, you need to install RAPIDS cuML.\n",
    "  - For specific installation info, please reference: https://docs.rapids.ai/install\n",
    "    - Make sure you install for the proper CUDA version (11.6 on Cheaha)\n",
    "    - Ensure you have the proper cupy version (cupy-cuda11x for Cheaha)\n",
    "  - Non-GPU accelerated versions of these packages are available if needed.\n",
    "\n",
    "## References:\n",
    "- https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html\n",
    "- https://medium.com/rapids-ai/faster-topic-modeling-with-bertopic-and-rapids-cuml-5c7559aba898\n",
    "- https://towardsdatascience.com/topic-modeling-with-lsa-plsa-lda-nmf-bertopic-top2vec-a-comparison-5e6ce4b1e4a5\n",
    "- https://hdbscan.readthedocs.io/en/latest/parameter_selection.html\n",
    "- https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/anna19/Conda_Env/nlp2023v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports compelete\n"
     ]
    }
   ],
   "source": [
    "# installs\n",
    "!pip install bertopic \n",
    "\n",
    "\"\"\"\n",
    "NOTE: to use GPU acclerated UMAP and HDBSCAN, you need to install RAPIDS cuML.\n",
    "Ensure you have the proper cupy version (cupy-cuda11x for Cheaha)\n",
    "For more installation info, see: https://docs.rapids.ai/install\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "\n",
    "# GPU accelerated\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import HDBSCAN\n",
    "\n",
    "# not GPU accelerated\n",
    "# from umap import UMAP\n",
    "# from hdbscan import HDBSCAN\n",
    "    \n",
    "print(\"imports compelete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and decompress data - run to initially get data\n",
    "\n",
    "- NOTE: if you have already downloaded a dataset using this cell, it is saved as a `json` file which can be used in the future. You do not need to run this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "chunk_size=5 * 1024 * 1024\n",
    "\n",
    "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\n",
    "\n",
    "with gzip.open('Books_5.json.gz') as f:\n",
    "    with open(\"Books_5.json\", 'wb') as f_out:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f_out.write(chunk)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data into list, concatenating the relevant items `summary` and `reviewText` for each review. Set the max number of reviews to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 2min 43s, sys: 22.1 s, total: 3min 6s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# maximum number of review to consider\n",
    "max_len = 1000000\n",
    "\n",
    "training_data_books = []\n",
    "\n",
    "with open('Books_5.json') as f:\n",
    "    for review in f:\n",
    "        review = json.loads(review)\n",
    "        text = review.get(\"reviewText\", \"\").strip()\n",
    "        summary = review.get(\"summary\", \"\").strip()\n",
    "        review = summary + \" \" + text\n",
    "        if review.strip():\n",
    "            training_data_books.append(review)\n",
    "        \n",
    "training_data = training_data_books[:max_len]\n",
    "            \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-calculate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 31250/31250 [11:48<00:00, 44.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 22s, sys: 1min 7s, total: 12min 29s\n",
      "Wall time: 12min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Generate embeddings for data, using a GPU if available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' \n",
    "    print(\"using gpu\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "embeddings = model.encode(training_data, device=device, show_progress_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data amount: 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 23:43:54,155 - BERTopic - Reduced dimensionality\n",
      "2023-11-26 23:44:47,139 - BERTopic - Clustered reduced embeddings\n",
      "2023-11-26 23:49:20,045 - BERTopic - Reduced number of topics from 246 to 246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained\n",
      "CPU times: user 8min 39s, sys: 2min 10s, total: 10min 49s\n",
      "Wall time: 10min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "print(f\"training data amount: {len(training_data)}\")\n",
    "\n",
    "# reduces dimensionality\n",
    "umap_model = UMAP(random_state=42)\n",
    "\n",
    "# does clustering\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=50)\n",
    "\n",
    "# remove stopwords, tokenize\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\", min_df = 5)\n",
    "\n",
    "\n",
    "keybert_model = KeyBERTInspired()\n",
    "pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"POS\": pos_model\n",
    "}\n",
    "\n",
    "bert_model = BERTopic(\n",
    "    embedding_model=model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    top_n_words=10,\n",
    "    nr_topics=500,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = bert_model.fit_transform(training_data, embeddings)\n",
    "\n",
    "bert_model.save(\"bertopic_model_books_1M_3\")\n",
    "\n",
    "print(\"model trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save visualizations of results, print info on each topic and its representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic   Count                                               Name  \\\n",
      "0       -1  393764                            -1_book_read_story_like   \n",
      "1        0     469          0_book stars_stars great_great book_stars   \n",
      "2        1     824          1_book stars_stars great_great book_stars   \n",
      "3        2     323  2_excellent stars_stars excellent_excellent_stars   \n",
      "4        3     649  3_excellent stars_stars excellent_stars good_b...   \n",
      "..     ...     ...                                                ...   \n",
      "241    240     319                240_minor_winchester_dictionary_oed   \n",
      "242    241  248561                           241_read_book_story_good   \n",
      "243    242     562                     242_trump_history_browne_urras   \n",
      "244    243    4210                  243_condition_arrived_cover_print   \n",
      "245    244     304          244_christina_wyeth_andrew wyeth_painting   \n",
      "\n",
      "                                        Representation  \\\n",
      "0    [book, read, story, like, good, just, life, gr...   \n",
      "1    [book stars, stars great, great book, stars, g...   \n",
      "2    [book stars, stars great, great book, stars, g...   \n",
      "3    [excellent stars, stars excellent, excellent, ...   \n",
      "4    [excellent stars, stars excellent, stars good,...   \n",
      "..                                                 ...   \n",
      "241  [minor, winchester, dictionary, oed, murray, d...   \n",
      "242  [read, book, story, good, great, characters, r...   \n",
      "243  [trump, history, browne, urras, people, water,...   \n",
      "244  [condition, arrived, cover, print, good condit...   \n",
      "245  [christina, wyeth, andrew wyeth, painting, pie...   \n",
      "\n",
      "                                               KeyBERT  \\\n",
      "0    [novel, book, read book, read, reading, reader...   \n",
      "1    [book stars, great book, read stars, stars gre...   \n",
      "2    [book stars, great book, brilliant book, stars...   \n",
      "3    [stars excellent, excellent stars, stars good,...   \n",
      "4    [stars excellent, excellent stars, stars great...   \n",
      "..                                                 ...   \n",
      "241  [william minor, mr winchester, dr minor, simon...   \n",
      "242  [novel, book read, great read, great book, rea...   \n",
      "243  [novel, book, read book, anarchist, author, re...   \n",
      "244  [received book, book great, book good, book ar...   \n",
      "245  [painting christina, christina world, christin...   \n",
      "\n",
      "                                                   MMR  \\\n",
      "0    [book, read, story, like, good, just, life, gr...   \n",
      "1    [book stars, stars great, great book, stars, g...   \n",
      "2    [book stars, stars great, great book, stars, g...   \n",
      "3    [excellent stars, stars excellent, excellent, ...   \n",
      "4    [excellent stars, stars excellent, stars good,...   \n",
      "..                                                 ...   \n",
      "241  [minor, winchester, dictionary, oed, murray, d...   \n",
      "242  [read, book, story, good, great, characters, r...   \n",
      "243  [trump, history, browne, urras, people, water,...   \n",
      "244  [condition, arrived, cover, print, good condit...   \n",
      "245  [christina, wyeth, andrew wyeth, painting, pie...   \n",
      "\n",
      "                                                   POS  \\\n",
      "0    [book, story, good, life, great, books, time, ...   \n",
      "1    [great book, great, book, bookk, recomment, , ...   \n",
      "2              [great book, great, book, , , , , , , ]   \n",
      "3                    [excellent, book, , , , , , , , ]   \n",
      "4       [good book, excellent, good, book, , , , , , ]   \n",
      "..                                                 ...   \n",
      "241  [minor, dictionary, asylum, son, book, creatio...   \n",
      "242  [book, story, good, great, characters, reading...   \n",
      "243  [trump, history, people, water, american histo...   \n",
      "244  [condition, cover, print, good condition, pric...   \n",
      "245  [painting, classic, edition, book, collection,...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [Not quite worth the price This comic was made...  \n",
      "1    [Five Stars Great book, Five Stars Great Book,...  \n",
      "2    [Five Stars great book, Five Stars great book,...  \n",
      "3    [Five Stars excellent, Five Stars Excellent, F...  \n",
      "4    [Five Stars excellent, Five Stars excellent, F...  \n",
      "..                                                 ...  \n",
      "241  [The Professor and the Madman What is the like...  \n",
      "242  [I really liked this book but...(SPOILER ALERT...  \n",
      "243  [Stay Focused, Maintain Momentum, and Aim for ...  \n",
      "244  [The book was in great condition. Thanks. The ...  \n",
      "245  [Beautifully written and read by the narrator....  \n",
      "\n",
      "[246 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "bert_model.visualize_topics().write_html(\"./intertopic_dist_model_800K.html\")\n",
    "bert_model.visualize_barchart(top_n_topics = 25).write_html(\"./barchart_model_800K.html\")\n",
    "bert_model.visualize_hierarchy().write_html(\"./hieararchy_model_800K.html\")\n",
    "\n",
    "print(bert_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
