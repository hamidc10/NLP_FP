{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation Instructions and References\n",
    "\n",
    "## Installation\n",
    "\n",
    "- BERTopic can be installed using pip.\n",
    "- To use GPU acceleration for UMAP and hDBSCAN, you need to install RAPIDS cuML.\n",
    "  - For specific installation info, please reference: https://docs.rapids.ai/install\n",
    "    - Make sure you install for the proper CUDA version (11.6 on Cheaha)\n",
    "    - Ensure you have the proper cupy version (cupy-cuda11x for Cheaha)\n",
    "  - Non-GPU accelerated versions of these packages are available if needed.\n",
    "\n",
    "## References:\n",
    "- https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html\n",
    "- https://medium.com/rapids-ai/faster-topic-modeling-with-bertopic-and-rapids-cuml-5c7559aba898\n",
    "- https://towardsdatascience.com/topic-modeling-with-lsa-plsa-lda-nmf-bertopic-top2vec-a-comparison-5e6ce4b1e4a5\n",
    "- https://hdbscan.readthedocs.io/en/latest/parameter_selection.html\n",
    "- https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports compelete\n"
     ]
    }
   ],
   "source": [
    "# installs\n",
    "# !pip install bertopic \n",
    "\n",
    "\"\"\"\n",
    "NOTE: to use GPU acclerated UMAP and HDBSCAN, you need to install RAPIDS cuML.\n",
    "Ensure you have the proper cupy version (cupy-cuda11x for Cheaha)\n",
    "For more installation info, see: https://docs.rapids.ai/install\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "\n",
    "# GPU accelerated\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import HDBSCAN\n",
    "\n",
    "# not GPU accelerated\n",
    "# from umap import UMAP\n",
    "# from hdbscan import HDBSCAN\n",
    "    \n",
    "print(\"imports compelete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and decompress data - run to initially get data\n",
    "\n",
    "- NOTE: if you have already downloaded a dataset using this cell, it is saved as a `json` file which can be used in the future. You do not need to run this cell again. If you want to download a particular dataset, comment out all except that one. Some of these datasets are rather large, so the process may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "## Download and decompress data\n",
    "\n",
    "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFilesSmall/Books_5.json.gz\n",
    "\n",
    "chunk_size=5 * 1024 * 1024\n",
    "\n",
    "with gzip.open('Books_5.json.gz') as f:\n",
    "    with open(\"Books_5.json\", 'wb') as f_out:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f_out.write(chunk)\n",
    "\n",
    "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFilesSmall/Home_and_Kitchen_5.json.gz\n",
    "with gzip.open('Home_and_Kitchen_5.json.gz') as f:\n",
    "    with open(\"Home_and_Kitchen_5.json\", 'wb') as f_out:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f_out.write(chunk)\n",
    "\n",
    "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFilesSmall/Sports_and_Outdoors_5.json.gz\n",
    "\n",
    "with gzip.open('Sports_and_Outdoors_5.json.gz') as f:\n",
    "    with open(\"Sports_and_Outdoors_5.json\", 'wb') as f_out:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f_out.write(chunk)\n",
    "\n",
    "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFilesSmall/Electronics_5.json.gz\n",
    "\n",
    "with gzip.open(\"Electronics_5.json.gz\", 'rb') as f:\n",
    "    with open(\"Electronics_5.json\", 'wb') as f_out:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f_out.write(chunk)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data into lists, concatenating the relevant items `summary` and `reviewText` for each review.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books done\n",
      "kitchen done\n",
      "outdoors done\n",
      "electronics done\n",
      "CPU times: user 6min 4s, sys: 28.4 s, total: 6min 33s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_data_books = []\n",
    "training_data_outdoors = []\n",
    "training_data_electronics = []\n",
    "training_data_kitchen=[]\n",
    "with open('Books_5.json') as f:\n",
    "    for review in f:\n",
    "        text = json.loads(review).get(\"reviewText\", \"\").strip()\n",
    "        summary = json.loads(review).get(\"summary\", \"\").strip()\n",
    "        review = summary + \" \" + text\n",
    "        if review.strip():\n",
    "            training_data_books.append(review)\n",
    "print(\"Books done\")\n",
    "    \n",
    "with open('Home_and_Kitchen_5.json') as f:\n",
    "    for review in f:\n",
    "        review = json.loads(review)\n",
    "        text = review.get(\"reviewText\", \"\").strip()\n",
    "        summary = review.get(\"summary\", \"\").strip()\n",
    "        review = summary + \" \" + text\n",
    "        if review.strip():\n",
    "            training_data_kitchen.append(review)\n",
    "            \n",
    "print(\"kitchen done\")\n",
    "\n",
    "with open('Sports_and_Outdoors_5.json') as f:\n",
    "    for review in f:\n",
    "        review = json.loads(review)\n",
    "        text = review.get(\"reviewText\", \"\").strip()\n",
    "        summary = review.get(\"summary\", \"\").strip()\n",
    "        review = summary + \" \" + text\n",
    "        if review.strip():\n",
    "            training_data_outdoors.append(review)\n",
    "            \n",
    "print(\"outdoors done\")\n",
    "\n",
    "with open('Electronics_5.json') as f:\n",
    "    for review in f:\n",
    "        review = json.loads(review)\n",
    "        text = review.get(\"reviewText\", \"\").strip()\n",
    "        summary = review.get(\"summary\", \"\").strip()\n",
    "        review = summary + \" \" + text\n",
    "        if review.strip():\n",
    "            training_data_electronics.append(review)\n",
    "print(\"electronics done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the number of reviews to consider from each topic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample review: Five Stars What a spectacular tutu! Very slimming.\n"
     ]
    }
   ],
   "source": [
    "num_book_reviews = 0\n",
    "num_electronics_reviews = 0\n",
    "num_outdoors_reviews = 500000\n",
    "num_kitchen_reviews = 500000\n",
    "\n",
    "training_data=[]\n",
    "training_data.extend(training_data_books[:num_book_reviews])\n",
    "training_data.extend(training_data_electronics[:num_electronics_reviews])\n",
    "training_data.extend(training_data_outdoors[:num_outdoors_reviews])\n",
    "training_data.extend(training_data_kitchen[:num_kitchen_reviews])\n",
    "\n",
    "print(f\"Sample review: {training_data[0]}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-calculate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 31250/31250 [05:05<00:00, 102.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 42s, sys: 6min 11s, total: 17min 54s\n",
      "Wall time: 5min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Generate embeddings for data, using a GPU if available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' \n",
    "    print(\"using gpu\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "embeddings = model.encode(training_data, device=device, show_progress_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data amount: 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 22:50:02,561 - BERTopic - Reduced dimensionality\n",
      "2023-11-30 22:50:51,241 - BERTopic - Clustered reduced embeddings\n",
      "2023-11-30 22:53:08,970 - BERTopic - Reduced number of topics from 244 to 244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained\n",
      "CPU times: user 6min 24s, sys: 2min 6s, total: 8min 30s\n",
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "print(f\"training data amount: {len(training_data)}\")\n",
    "\n",
    "# reduces dimensionality\n",
    "umap_model = UMAP(random_state=42)\n",
    "\n",
    "# does clustering\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=50)\n",
    "\n",
    "# remove stopwords, tokenize\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\", min_df = 5)\n",
    "\n",
    "\n",
    "keybert_model = KeyBERTInspired()\n",
    "pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"POS\": pos_model\n",
    "}\n",
    "\n",
    "bert_model = BERTopic(\n",
    "    embedding_model=model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    top_n_words=10,\n",
    "    nr_topics=500,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = bert_model.fit_transform(training_data, embeddings)\n",
    "\n",
    "bert_model.save(\"bertopic_model_sports_outdoors_1M\")\n",
    "\n",
    "print(\"model trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save visualizations of results, print info on each topic and its representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic   Count                                         Name  \\\n",
      "0       -1  532506                      -1_stars_great_good_use   \n",
      "1        0    1574        0_great stars_stars great_stars_great   \n",
      "2        1     869           1_good stars_stars good_stars_good   \n",
      "3        2     331           2_good stars_stars good_stars_good   \n",
      "4        3     976  3_great stars_stars works_works great_works   \n",
      "..     ...     ...                                          ...   \n",
      "239    238     382              238_sturdy_rack_clothes_garment   \n",
      "240    239   32226               239_product_quality_good_price   \n",
      "241    240     380                 240_washer_dryer_shelf_color   \n",
      "242    241    2716                      241_cake_pan_bacon_pans   \n",
      "243    242   10625                  242_pan_cast iron_cast_iron   \n",
      "\n",
      "                                        Representation  \\\n",
      "0    [stars, great, good, use, just, like, product,...   \n",
      "1    [great stars, stars great, stars, great, , , ,...   \n",
      "2    [good stars, stars good, stars, good, love sta...   \n",
      "3    [good stars, stars good, stars, good, perfect ...   \n",
      "4    [great stars, stars works, works great, works,...   \n",
      "..                                                 ...   \n",
      "239  [sturdy, rack, clothes, garment, garment rack,...   \n",
      "240  [product, quality, good, price, works, great, ...   \n",
      "241  [washer, dryer, shelf, color, gift, washer dry...   \n",
      "242  [cake, pan, bacon, pans, cakes, cake pan, baki...   \n",
      "243  [pan, cast iron, cast, iron, skillet, pans, no...   \n",
      "\n",
      "                                               KeyBERT  \\\n",
      "0    [stars great, stars, great product, pan, bag, ...   \n",
      "1    [stars great, great stars, stars, great, , , ,...   \n",
      "2    [stars good, stars great, good stars, great st...   \n",
      "3    [stars good, good stars, stars perfect, stars,...   \n",
      "4    [stars works, stars great, stars good, stars, ...   \n",
      "..                                                 ...   \n",
      "239  [sturdy sturdy, pretty sturdy, sturdy great, s...   \n",
      "240  [excellent product, product great, great produ...   \n",
      "241  [washer dryer, washer, dryer, dryer just, shel...   \n",
      "242  [cake pan, cake pans, baking pan, baking pans,...   \n",
      "243  [iron pan, iron skillet, grill pan, frying pan...   \n",
      "\n",
      "                                                   MMR  \\\n",
      "0    [stars, great, good, use, just, like, product,...   \n",
      "1    [great stars, stars great, stars, great, , , ,...   \n",
      "2    [good stars, stars good, stars, good, love sta...   \n",
      "3    [good stars, stars good, stars, good, perfect ...   \n",
      "4    [great stars, stars works, works great, works,...   \n",
      "..                                                 ...   \n",
      "239  [sturdy, rack, clothes, garment, garment rack,...   \n",
      "240  [product, quality, good, price, works, great, ...   \n",
      "241  [washer, dryer, shelf, color, gift, washer dry...   \n",
      "242  [cake, pan, bacon, pans, cakes, cake pan, baki...   \n",
      "243  [pan, cast iron, cast, iron, skillet, pans, no...   \n",
      "\n",
      "                                                   POS  \\\n",
      "0    [stars, great, good, use, product, nice, easy,...   \n",
      "1                            [great, , , , , , , , , ]   \n",
      "2                             [good, , , , , , , , , ]   \n",
      "3    [good, good eyes, picture, eyes, perfect, , , ...   \n",
      "4                            [great, , , , , , , , , ]   \n",
      "..                                                 ...   \n",
      "239  [sturdy, rack, clothes, garment, solid, heavy,...   \n",
      "240  [product, quality, good, price, great, good qu...   \n",
      "241  [washer, dryer, shelf, color, gift, sign, cute...   \n",
      "242  [cake, pan, bacon, pans, cakes, baking, muffin...   \n",
      "243  [pan, cast iron, cast, iron, skillet, pans, no...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [Yes, You Can Fall In Love With A Dish Rack! A...  \n",
      "1    [Five Stars great, Five Stars GREAT, Five Star...  \n",
      "2    [Five Stars GOOD, Five Stars good, Five Stars ...  \n",
      "3    [Four Stars good, Four Stars Good, Four Stars ...  \n",
      "4    [Five Stars Works great, Five Stars works grea...  \n",
      "..                                                 ...  \n",
      "239  [Best Garment Rack! This is the third garment ...  \n",
      "240  [great product at a great price fast service, ...  \n",
      "241  [Great Solution My prior homeowner left one of...  \n",
      "242  [Excellent, Medium Weight, USA-Made Pan... Nor...  \n",
      "243  [A terrific companion for our Lodge cookware W...  \n",
      "\n",
      "[244 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "bert_model.visualize_topics().write_html(\"./intertopic_dist_sports_outdoors_1M.html\")\n",
    "bert_model.visualize_barchart(top_n_topics = 25).write_html(\"./barchart_sports_outdoors_1M.html\")\n",
    "bert_model.visualize_hierarchy().write_html(\"./hieararchy_sports_outdoors_1M.html\")\n",
    "\n",
    "print(bert_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2]",
   "language": "python",
   "name": "conda-env-nlp2023v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
